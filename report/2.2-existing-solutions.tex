\section{Solutions for overproducing sources}
In the previous chapter we presented a number of solutions for overproduction, ranging from putting data in an overflow buffer to gaining more control over the \obs. Some of these solutions work perfect under certain circumstances, others do not. In this section we will discuss the solutions that were described in \Cref{sec:fastproc-slowcons} in further detail and reflect on them in the light of the distinction between hot and cold streams.

\subsection{Avoiding overproduction}
As described in \Cref{subsec:avoiding-overproduction}, \textit{avoiding} is a first line of defense for overproduction. This is done by introducing several operators that either propagate only a portion of the data and drop the rest or buffer the data and propagating these buffers downstream for further processing. All lossy operators, as well as the \code{buffer} with interval, require a \sch for them to run their interval timers on, hence the output stream of these operators runs on a different thread.

For a hot \obs this kind of defense mechanism is perfect, as the speed at which the source is producing is unknown. This also holds for the cold asynchronous \obs, as the production of elements is also bound to a notion of time. For other cold streams (like \code{Observable(1, 2, 3, 4)}) it does not make any sense to add any overproduction-avoiding operators, as this kind of stream is sequential and will in fact emit at a rate which is determined by the downstream (this is discussed in further detail in the next section).

We are aware of the fact that these operators also have use cases other than avoiding overproduction, such as edge detection or calculating the derivative of a stream of numbers. However, these use cases are not relevant for this thesis.

\subsection{Callstack blocking}
Subscribing to an \obs is basically nothing more than supplying an \obv to the function within \code{Observable.create} and \emph{sequentially} executing this function using this provided \obv. Once an element is emitted by the source, all operations in the \obs sequence are executed on that element before a second element is emitted. In the sequence of operators in \Cref{lst:operators-obs}, first all operations on $1$ are performed, before $2$ is emitted by the \obs (and then discarded by \code{filter}). This is also the case in \Cref{lst:observeOn} up to \cref{line:observeOn-in-observeOn}, after which the elements are scheduled on a different thread and hence are further processed in parallel with emitting new elements from the source.

The order of operations is designed in such a way that it allows for lazy evaluation. If a cold \obs contains 5 elements and the operator sequence contains the operator \code{take(2)} (see \Cref{lst:lazy}), only the first 2 elements from this \obs will be evaluated by the upper half of the operator sequence, rather than all 5. After the second element has passed \code{take}, it sends an \code{onCompleted()} downstream, right after the second \code{onNext}, causing the stream to end without evaluating the other 3 elements.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Lazy evaluation}, label={lst:lazy}]
Observable(1, 2, 3, 4, 5)
    // some operators
    .take(2)
    // some more operators
    .subscribe(i $\Rightarrow$ print(i + " ")) // only prints the first and second element
\end{lstlisting}
\end{minipage}

Following this order of operations, we can conclude that basically every operator in the operator sequence is in a sense blocking the callstack during its computation and thereby preventing the source from emitting a next element until the previous element has gone through the whole operator sequence. This is the reason why we concluded in the previous section that avoiding overproduction on a cold synchronous stream does not make any sense. The rate of emission is already determined by the speed in which all operators together are executed.

This is also true for all kind of stream, whether hot or cold, whether or not it has latency and no matter what kind of source emits the data. This may seem quite surprising, especially for the hot \obs, since we always claimed that this type of stream is only controlled by its outside environment. However, an example of this is shown in \Cref{lst:blocking-hot-obs}. Here a cold \obs is subscribed to a \subj (\cref{line:blocking-hot-subject-subscribe}), making it hot according to \Cref{subsec:subjects}. At several points in the operator sequence the time passed since \code{start} is measured and printed. At \cref{line:blocking-hot-sleep} the callstack is blocked for 1 second by pausing the thread on which this whole process is running. The console output of executing \Cref{lst:blocking-hot-obs} is provided in \Cref{lst:console-output-blocking-hot}\footnote{Due to the inner workings of the JVM, the times shown here may vary by a couple of milliseconds in every execution.}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Applying callstack blocking on a hot \obs}, label={lst:blocking-hot-obs}]
def now $=$ System.currentTimeMillis()
val start $=$ now
def timePassed $=$ now - start

val timer $=$ Observable(1, 2, 3, 4)
    .tee(i $\Rightarrow$ println("[" + timePassed + "] emitted - " + i))
val subject $=$ Subject[Int]()

subject.tee(i $\Rightarrow$ println("[" + timePassed + "] before - " + i))
    .tee(_ $\Rightarrow$ Thread.sleep(1000)) |\label{line:blocking-hot-sleep}|
    .subscribe(i $\Rightarrow$ println("[" + timePassed + "] after - " + i))
timer.subscribe(subject) |\label{line:blocking-hot-subject-subscribe}|
\end{lstlisting}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Console output from \Cref{lst:blocking-hot-obs}}, label={lst:console-output-blocking-hot}]
[47] emitted - 1
[47] before - 1
[1059] after - 1
[1059] emitted - 2
[1059] before - 2
[2060] after - 2
[2060] emitted - 3
[2060] before - 3
[3062] after - 3
[3062] emitted - 4
[3062] before - 4
[4064] after - 4
\end{lstlisting}
\end{minipage}

From these measurements we see that the first value was emitted at $t=47\ ms$. Almost immediately after that, the thread on which the whole program is running is blocked for 1 second. At time $t=1059\ ms$ the first value is propagated to the \code{subscribe}. \emph{Only then} the second item is emitted by \code{Observable.apply}.

From this we can conclude that even a hot \obs can be controlled by callstack blocking. Notice however that this creates a (potentially unbounded) buffer of unprocessed \code{onNext} calls within the \code{Observable.create}'s callstack, using an excessive amount of memory. With that we only emulate the naive solution to overproducing streams (see \Cref{sec:fastproc-slowcons}) by using callstack blocking. The same buffering behavior can also happen within the \code{observeOn} operator, when the other thread used callstack blocking to slow down the stream.

The case described above is one where the thread cannot be blocked safely. It can potentially blow up the program when the buffer gets too big. This is what RxJava warns against in its wiki \cite{RxJava-Wiki-Callstack-Blocking} and what RxMobile warns against in the context of the particular \code{zip} implementation discussed in \Cref{subsec:callstack-blocking} \cite{RxMobile}. Only certain kinds of operators can use callstack blocking safely, provided with streams that can handle this callstack blocking safely.

Another issue with a hot \obs is what happens when more than one \obv is subscribed and both do callstack blocking. This can potentially lead to even more disastrous situations, where a fast \obv suffers from a slower one and where deadlock situations are inevitable.

In general we can conclude that controlling the flow of data by callstack blocking is already implicitly used in cold no-latency streams, but that there is no good in using it on a hot or cold with-latency \obs, unless being completely certain of the stream's behavior.

\subsection{Reactive Streams}
\label{subsec:handling-overproduction-with-reactive-streams}
A third way of managing the overflow of data is by using the backpressure technique that was found by the Reactive Streams initiative. The associated API, discussed in \Cref{subsec:reactive-streams}, consists of a \code{Publisher} that corresponds to the Rx \obs, a \code{Subscriber} that adds an \code{onSubscribe} method to the Rx \obv and a \code{Subscription} with a \code{cancel} and \code{request} method that replaces the Rx \subs. The intention of this API is to let the consumer be in charge rather than the producer. It is the consumer that dictates how many elements it wants to receive and with that it follows the design of the TCP protocol (see \Cref{subsec:tcp}) as the receiver also sends window size updates to the sender.

The most important question to ask here is whether or not the approach of Reactive Streams is a solution to overproducing sources in reactive programming, as is advertised on the website \cite{Reactive-Streams} and implied by the name. To answer this question, we first need to reflect on the previous paragraph: the consumer is in charge and the producer can at most send the amount of data that is requested by the consumer. Following the definitions from Albert Benveniste and G\'erard Berry \cite{berry1991-Reactive} on reactive and interactive programs (see \Cref{sec:reactive-programming}), we must conclude that this API creates an \emph{interactive} program, that ``\textit{interacts at its own speed with users or with other programs}'', in this case with the \code{Producer}.

Besides that, we need to take into consideration that `reactive' is the dual of `interactive'. In other words, an interactive interface needs to be dualized in order to become reactive. However, the Reactive Streams API can be constructed from \ieb without using any dualization techniques, as shown by Erik Meijer during a conference talk at Lambda Jam 2014 \cite{meijer2014-Derivation}. Instead he shows that a combination of techniques such as coproduct, continuation passing style and currying will suffice. From this he concludes that the Reactive Streams API is not reactive at all, but is interactive instead.

Reactive Streams, even if it is interactive rather than reactive, still distinguishes itself from the classical set of interactive programming interfaces: \ieb and \ier. Rather than calling \code{moveNext} and \code{current}, a \code{Subscriber} can advertise to its \code{Producer} that it can handle \code{n} more element. It is then up to the latter to send these elements either immediately or after some amount of delay to the former by calling the \code{onNext} method, signaling the end of the stream by calling \code{onCompleted} or propagating an exception by calling \code{onError}. Just as Rx, the API is set up in such a way that it does not block the program flow, which would be the case when using \ieb and \ier. In that sense, Reactive Streams is definitely not the same as \ieb and \ier, but is rather an API for \textit{asynchronous interactive programming}; it is not just a normal pull model, it is an \textit{asynchronous} pull model.

The question of whether Reactive Streams is able to solve the regulation of overproducing sources in reactive programming still remains. As we understand now what the API really is (an asynchronous pull model), we can finally reason about what kind of sources it can wrap and with that replace the Rx API.

Both a synchronous and asynchronous cold sources are perfect candidates to be wrapped in this kind of stream as they are already interactive by nature. The \code{Subscriber} can pull as much data from the source as it wants, only restricted by the size of the source in case it is finite. Note that for a synchronous source the response of a request will be immediate, whereas the asynchronous source might take some arbitrary amount of time to produce its next value.

A hot source on the other hand is not as suitable for the Reactive Streams API as the cold sources are. This is due to the fact that there is no way for the \code{Producer} in which the hot source is wrapped to interact with it. It cannot request the next \code{n} elements from it, nor can it request to slow down or speed up. This is implied in the nature of a hot source\cite{berry1991-Reactive}. The only way to have a hot source be wrapped in a \code{Producer} is to drain its the data in a buffer or queue and send the requested amount of elements from this buffer to the \code{Subscriber}. In general this is again that dangerous move of possibly storing unbounded amounts of data for later processing as is described in earlier sections. That being said, in special cases, where in the long term the downstream can keep up with the source, it is in fact possible to use this API for it. An example of this may be a source with bursty behavior, where in a short amount of time a large amount of data is produced, after which no data is produced for a longer amount of time than the consumer requires for the data to be processed.

\subsection{RxJava and reactive pull}
\label{subsec:handling-overproduction-with-rxjava}
RxJava started of as a port of Rx.Net for the JVM and was until version 0.20 purely reactive. It did not have any other policies for handling backpressure than the ones discussed in \Cref{subsec:avoiding-overproduction}. In version 0.20 backpressure support was introduced as a result of collaborations in the Reactive Streams initiative. This implementation (also referred to as \textit{reactive pull}) did not change the original Rx interfaces but rather added extra interfaces and methods that were derived from the Reactive Streams API. Due to these changes, the reactiveness is partially lost, as the data flow is now controlled by \code{request(n)} calls from the \obv.

From version 0.20 onward, RxJava has a lot of similarities with the Reactive Streams API in terms of handling backpressure. It is therefore well equipped to handle cold sources by wrapping them in the \obs. However, in contrast to Reactive Streams, this API is still able to correctly coop with hot sources, even though it explicitly declares that backpressure is \emph{not} suitable for this kind of stream \cite{RxJava-Wiki-HotCold}. The RxJava wiki states that for this to be enabled, the \obs needs to be initialized with \code{request(Long.MAX\_VALUE)}, which orders the \obs to emit data at its own pace \cite{RxJava-Wiki-Backpressure}. It also advises to use other flow control strategies in general for a hot stream, mainly the ones discussed in \Cref{subsec:avoiding-overproduction}, as well as the \code{onBackpressureDrop} and \code{onBackpressureBuffer} operators that were mentioned in \Cref{subsec:reactive-pull}.

While introducing backpressure support to RxJava, it turned out that all operators that needed to support this feature had to be reimplemented for the \code{request(n)} to be used. In some cases (like \code{take(n)} or \code{skip(n)}) this can be a simple straightforward refactoring or an extra method override. However, in other cases it is much more complicated to do this. Since the \obs is a monad, it has to implement \code{flatMap}, which in the world of streams means mapping and merging. But how can one implement this \code{merge} operator in terms of requests? When \code{n} elements are requested, to which of the \code{m} upstream \obs sequences is this request sent? Is it sent to only \emph{one} of these, with the possibility that this is a rather slow producer; or is the \code{request(n)} split into multiple smaller \code{request(k)}s and sent to multiple upstreams with the possibility of getting less than \code{n} results; or is the \code{request(n)} sent to all upstreams with the possibility of getting much more than \code{n} elements? All of a sudden the implementation is no longer straightforward and easy, as we would like it to be, but becomes much more complicated by making decisions like this. RxJava chose an approach with round-robin techniques to implement \code{merge}. This however caused a giant increase in code complexity \cite{RxJava-source-code}, making the code almost unreadable and not suitable for any form of maintenance.

Similar problems with timing, mouse moves and operators like \code{groupBy} are discussed in \cite{meijer2014-Derivation}.

The underlying issue of this code complexity is the coupling of this approach with the operators. When the new strategy of backpressure needed to be introduced in the 0.20 version of the RxJava code base, it automatically followed that the operators had to be changed as well. This was mainly caused by change in direction of control, which went from `\textit{the producer is in charge}' to `\textit{the consumer is in charge}'.

As backpressure is mainly suitable for cold sources, not all operators are able to support backpressure, since they convert to hot streams or use another flow control mechanism like time or another stream. Detailed reasons per operator for not implementing backpressure can be found in the RxJava documentation \cite{rx-api}.

\subsection{Conclusion}
In general we can conclude that it differs per situation what can be done to avoid overproduction. We can conclude that by definition a hot source does not allow for interaction, neither does it provide any way to request it to either speed up or slow down. Backpressure as invented by the Reactive Streams collaboration and used in RxJava does not take this kind of sources into account. The only way to make these two work together is to drain the source in a buffer and wrap the Reactive Streams \code{Producer} or an Rx \obs around this buffer. Unless preventive actions are taken, this can cause an unwieldy amount of data being stored in memory, which is exactly the problem that backpressure is trying to solve.

The best solution for managing a stream and guarding for overproduction in cases of a hot stream is either bundling the data in larger groups and processing it in these groups or discarding some of the data that cannot be processed immediately or shortly after it was received. These solutions are already present in the Rx API as the lossy and lossless operators discussed before.

Cold sources on the other hand, regardless of being synchronous or asynchronous, work well and are better suited for techniques like backpressure. With these sources the consumer can leverage their interactiveness by advertising to the producer how much data it is currently willing to accept. For these sources the Reactive Streams API works fine and is well suited. The fact that this causes the whole stream to being not reactive by definition is not important here, since the source itself is not reactive.