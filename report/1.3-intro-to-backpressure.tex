\section{Fast producers, slow consumers}
\label{sec:fastproc-slowcons}
In the previous section we discussed that a reactive collection is equivalent to any interactive collection: it obeys the same rules and the same operators (like \code{map} and \code{filter}) can be defined. One difference however is that a reactive collection is push-based, whereas an interactive collection is pull-based. Rather than the consumer being in charge, asking for the next value, here the producer is in charge and \emph{it} decides when to emit a next value. The consumer just has to listen and can only react to the elements emitted by the producer. This is conform the definition of a reactive collection in section~\ref{sec:reactive-programming}: ``\textit{Reactive programs maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not by the program itself.}''.

A risk that arises from allowing the producer to be in charge occurs when the consumer cannot keep up with the rate in which the producer is producing the data. This gives rise to the problem of what to do with the growing accumulation of unconsumed data.

A classic example of overproducing observables is the \code{zip} operator, which merges two (or more) streams by using a combiner function whenever both streams have produced an element. In this operator the problem occurs when one \obs always produces faster than the other. This is a problem, because \code{zip} cannot keep up with the rate in which the first \obs is producing, since the second \obs is much slower. The most common, but also slightly naive, implementation for this operator maintains an ever expending buffer of elements emitted by the first \obs to eventually combine them with the data emitted by the slower stream.

Ever expanding buffers is common answer to overproducing observables. A fast emitting stream is draining its data into a buffer and the much slower \obv eventually takes the data out of the buffer. The major problem with this solution (and thus with the implementation of \code{zip} as described above) is that the buffer will have to use an unwieldy amount of system resources.

\subsection{Avoiding overproduction}
\label{subsec:avoiding-overproduction}
One way of dealing with overproducing observables is to simply avoid the problem and take proactive measures whenever it is expected happen. In the following we will discuss several ways to reduce the amount of data by using standard operators that are defined on the \obs interface. For this we distinguish two types of operators: lossy and lossless operators \cite{Backpressure-Explained}.

\subsubsection{Lossy operators}
One set of operators avoids the problem in a lossy way, meaning that some of the emitted data will be dropped.

\paragraph{Throttling} The \code{throttle(interval: Duration)} operator only propagates the first element that is received in a particular interval. All other elements are discarded. Once an interval has finished, a new interval starts immediately, in which again the first received element is propagated and all other elements are discarded.

\paragraph{Sample} Rather than propagating the first element and discarding all others, the \code{sample(interval: Duration)} operator is used to discard all elements except for the last one that is received in a certain \code{interval}. This operator can for example be used when someone only wants to receive the data from a stock ticker every 5 seconds, without the need to process every value that comes in between.

\paragraph{Debounce} The operator \code{debounce(timespan: Duration)} will only propagate its received values after a certain \code{timespan} has passed without receiving an other value. When a value is received within the \code{timespan}, the previous value is discarded and the same process starts all over again with the newly received value. This operator is most commonly used in text fields within user interfaces, in order to avoid too much keystroke events being generated by a fast typing user. Rather than every keystroke being emitted, this operator will only yield the last keystroke after a particular \code{timespan}. Notice that in some versions of Rx this operator is also referred to as \code{throttleWithTimeout}.

With these operators many situations of potentially overproducing observables can be avoided by eliminating all the elements that do not really matter for a particular application. Note that, since these operators are depending on intervals, they have to be subscribed on a \sch, which is supplied as an extra argument to either one of these operators. In case no \sch is provided, a default instance will be used which differs per implementation of Rx.

\subsubsection{Loss-less operators}
Even though these lossy operators solve a large part of the problem, there are still as many cases left in which \emph{all} data that is send over a stream needs to be processed. For these kinds of use cases Rx provides so-called loss-less operators. These operators can buffer or group the data and emit collections of data, such that there are at least less elements to deal with.

\paragraph{Buffer} The \code{buffer} operator comes in a couple of different overloads. First of all, the \code{buffer(n: Int): Observable[List[T]]} operator, which receives data and groups it into lists of \code{n} elements. Once a list is filled (its size is \code{n}), it is emitted to downstream. Until then, the operator holds the data it receives. The \code{buffer} operator also comes in another form: \code{buffer(interval: Duration)}, which groups the data it receives within a certain interval into a single list. Just as with the lossy operators, note that this operator depends on an interval and therefore requires a \sch. Finally there is \code{buffer[B](boundary: Observable[B])}, which groups the data it receives between two emissions of \code{boundary} into a single list.

\paragraph{Window} The drawback of a buffer is that it only propagates its received values once the buffer is filled, the boundary \obs fires or the interval is over. All the time in between no elements will be received by the observer or downstream operators. This already becomes clear from the return type: \code{Observable[List[T]]}. In order to accomplish the same behavior but with an \obs rather than a \code{List}, the \code{window} operator is included in Rx, having the same overloads as \code{buffer}. Instead of the list being emitted only once it is completely filled, the inner \obs is emitted as soon as the first element is received and is \emph{completed} once the size, interval or boundary requirement is met.

These operators form a first line of defense against overproduction. For streams where not all elements necessarily need to be processed (for instance the keyboard events on a text field), a lossy operator can be used. For streams where all emitted data is needed, a loss-less operator is the right solution. Besides that, for special occasions lossy and loss-less backpressure operators can be combined in order to create the optimal buffering strategy. This was discussed in some further detail in a conference talk at QCon by Ben Christensen \cite{christensen2014-RxServiceArchitecture}.

\subsection{Callstack blocking}
\label{subsec:callstack-blocking}
Another way of dealing with this problem is to block the callstack and with that `park' the thread on which the \obs is running. This directly slows down the producer and gives the consumer more time to process each element of the stream. Despite the fact that this approach goes against the `reactive' and `non-blocking' model of Rx, it can potentially be a viable option if the overproducing \obs runs on a thread that can safely be blocked.

This technique is currently not used in RxJava\cite{RxJava-Wiki-Callstack-Blocking}, but \emph{is} used in a particular implementation of the \code{zip} operator in RxMobile \cite{RxMobile}. Once either one of the streams emits a value, it is blocked until the other \obs has produced a value as well. In order to avoid blocking the upstream callstack completely, it is strongly recommended to switch each \obs to another thread or scheduler using the \code{observeOn} operator. With this only the callstack is blocked upto the start of this new thread or scheduler. Once the other stream has produced a value, the blocking of the first \obs sequence is removed and the \code{zip} operator waits until either one of the sources emits a next value.

\subsection{Reactive Streams}
\label{subsec:reactive-streams}
Reactive Streams is an initiative \cite{Reactive-Streams} of a number of companies such as Netflix, Pivotal and Typesafe with the mission to \textit{provide a standard for asynchronous stream processing with non-blocking backpressure}. This collaboration resulted in an alternative API \cite{Reactive-Streams-API} for stream processing that is claimed to be capable of handling overproducing observables using backpressure.

This Reactive Streams API (see \Cref{lst:pub-sub}) looks fairly similar at first glance to the original API that was developed by Microsoft, but has some particular differences that have significant influence on the handling of backpressure. The \obs, renamed \code{Publisher}, looks the same: it still is parameterized over \code{T} and still has a \code{subscribe} method. The \obv, which was passed to the \code{subscribe} method, is replaced with a \code{Subscriber}, even though it is almost the same\footnote{The \code{onComplete} method in \Cref{lst:pub-sub} does not contain a typing error compared to \Cref{lst:obs-obv}. This is actually how the API is specified!}. It only adds an extra method \code{onSubscribe}, which is called right after the \code{Subscriber} is subscribed to the \code{Publisher}. This \code{onSubscribe} method requires an argument of type \code{Subscription}. This last interface contains two methods: \code{cancel}, which is similar to the \code{unsubscribe} method in \Cref{lst:obs-obv} and a new method \code{request(n: Long)}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Publisher, Subscriber and Subscription}, label={lst:pub-sub}]
trait Publisher[T] {
    def subscribe(s: Subscriber[T]): Unit
}

trait Subscriber[T] {
    def onNext(t: T): Unit
    def onError(e: Throwable): Unit
    def onComplete(): Unit
    def onSubscribe(s: Subscription): Unit
}

trait Subscription {
    def cancel(): void
    def request(n: Long): Unit
}
\end{lstlisting}
\end{minipage}

This last method reveals the whole idea behind the Reactive Streams API, namely to let the \code{Subscriber} request a certain amount of elements from the \code{Publisher}. This way the \code{Subscriber} is in charge of how many elements it will receive eventually and the \code{Publisher} just has to send at most this amount of elements by calling the \code{onNext} method.

Taking a step back and evaluating the true meaning of what the Reactive Streams collaboration has come up with, results in the conclusion that this API is interactive rather than reactive, since it lets the consumer (\code{Subscriber}) be in charge of the rate at which data is sent to him. This is discussed in more detail in a conference talk at Lambda Jam 2014 by Erik Meijer \cite{meijer2014-Derivation}. Compared to the earlier discussed \ieb collections, it only adds the features of non-blocking and requesting more than one element (which is what the \ieb interface technically does) per request. We will discuss the consequences of this API in more detail in a later section.

\subsection{Reactive pull}
\label{subsec:reactive-pull}
The ideas that sprout from the Reactive Streams initiative have been incorporated in the RxJava library. They kept the original naming conventions of \obs and \obv but added a couple of new methods to the latter: \code{request(n: Int)}, which signals to the \obs that it will be able to handle \code{n} new elements and \code{onStart()}, which performs the initial request from the \obv to the \obs. After the initial request is done, the \obs sends at most \code{n} elements to the \obv, which receives them in the \code{onNext} method. This is now the place where the \obv can call \code{request} again to receive more data.

We recognize these two methods from the Reactive Streams API, where \code{onStart()} was called \code{onSubscribe} and \code{request(n: Long)} was inside the \code{Subscription} interface. Making these slight modifications does not change the intention of the Reactive Streams API: it introduces a feature called \textit{reactive pull} in the \obv to manage the number of elements that are emitted by the upstream \obs.

Earlier versions of RxJava, who did not have this feature only had the ability to communicate upstream by calling the \code{unsubscribe} method. Recall that when an unsubscribe occurs, the \obs is basically signaled to stop emitting any data to the \obv. Besides unsubscribing there was no way in the standard Rx model to communicate upstream.

This feature from Reactive Streams allows the \obv to have some more control by \emph{pulling} from the data source (\obs) at its own pace. RxJava is set up in such a way that processing data under normal circumstances is still push based. Only when the \obv can't handle the speed in which data is sent, it will switch to this pull based model. Wrapping it up in this way, the problem of overproduction is not prevented or gone away but is rather moved up the chain of operators to a point where it can be handled better \cite{RxJava-Wiki-Backpressure}.

With this it limits the number of elements that are in a buffer within certain operators. Using this new feature, the earlier mentioned \code{zip} operator is implemented by using a small buffer for each \obs. It only requests items from one of these sources when there is room for more elements in its buffer. Once all buffers contain at least one element, the operator can remove an element from each buffer, zip these together and push them downstream. After that there is room for at least one extra element in all buffers, hence new requests are sent to the upstream.

Notice that the RxJava wiki \cite{RxJava-Wiki-Backpressure} points out that this method only works when all streams that are zipped together respond correctly to the \code{request()} method. This is \emph{not} a requirement for the normal \obs, but it is required for instances of \obs that are used in operators like \code{zip} that depend on reactive pull. RxJava therefore provides operators such as \code{onBackpressureBuffer} and \code{onBackpressureDrop} that respectively buffer and drop data that cannot be consumed immediately by the downstream \obv.

\subsection{Transmission Control Protocol}
\label{subsec:tcp}
A well known protocol from the Internet protocol suite is the Transmission Control Protocol (TCP), which offers reliable delivery of streams of data between applications communicating over an IP network \cite{tanenbaum2011-Computer-Networks}. In order to avoid having a sender to send data too fast for the TCP receiver to receive and process the data, it uses an end-to-end flow control protocol with a sliding window. The receiver specifies in the TCP header's \textit{Window Size} what amount of additional data it is willing to receive and buffer for the current connection. The sending host can only send at most that amount of data and has to wait for an acknowledgment and window update from the receiving host.

In case the sender receives a window size of 0, it obviously cannot send any data and instead starts a so called \textit{persist timer} that protects the TCP connection from a permanent deadlock. When this timer expires without the sender having received any new window sizes, it will send a small packet in order of the receiver to send a new acknowledgment containing a new window size.

The problem solved by this flow control protocol is in fact the same as overproducing observables discussed in this section. The solution presented by TCP is similar to \textit{reactive pull} and \textit{Reactive Streams} in the sense that they too advertise an `available buffer space message' to the \code{Producer} or \obs by calling the \code{request} method. The difference with TCP is that the earlier discussed solutions do not have the need for a persist timer since deadlock from lost window updates is not possible.
